# åŒéšå±¤:
#   - environment.jsonï¼ˆAPI ã‚­ãƒ¼æ ¼ç´ï¼‰
#   - vtuber_list.csvï¼ˆã‚«ãƒ©ãƒ : channel_urlï¼‰
# å‡ºåŠ›:
#   - vtuber_analytics_summary.csv
#
# ä½¿ã„æ–¹: ãã®ã¾ã¾å®Ÿè¡Œï¼ˆå¼•æ•°ãªã—ï¼‰ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†’é ­ã®å®šæ•°ã‚’ç·¨é›†ã—ã¦ä½¿ã†ã€‚

import csv, time, re, json
from urllib.parse import urlparse
from datetime import datetime, timedelta, timezone
from googleapiclient.discovery import build

api_key_json = json.load(open("./environment.json", "r"))
API_KEY = api_key_json["vtuber_analytics_api_key"]

# ====== ã“ã“ã‚’ç·¨é›† ======
DATE_FROM = "2025-09-30"           # æœŸé–“é–‹å§‹ (yyyy-mm-dd)
DATE_TO   = "2025-10-06"           # æœŸé–“çµ‚äº† (yyyy-mm-dd, å½“æ—¥ã‚’å«ã‚€)
INPUT_CSV = "vtuber_list.csv"
OUTPUT_CSV = "vtuber_analytics_summary.csv"
# ========================

YOUTUBE = build("youtube", "v3", developerKey=API_KEY)

def to_rfc3339_utc(date_str_from: str, date_str_to: str):
    """
    'yyyy-mm-dd' ã‹ã‚‰ RFC3339 UTC æ–‡å­—åˆ— (publishedAfter, publishedBefore) ã‚’ä½œæˆã€‚
    çµ‚äº†æ—¥ã¯ã€Œç¿Œæ—¥00:00:00Zã€ã‚’ before ã¨ã—ã¦åŠé–‹åŒºé–“ã«ã™ã‚‹ã“ã¨ã§å½“æ—¥ã‚’å«ã‚ã‚‹ã€‚
    """
    d0 = datetime.strptime(date_str_from, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    d1 = datetime.strptime(date_str_to, "%Y-%m-%d").replace(tzinfo=timezone.utc) + timedelta(days=1)
    return d0.isoformat().replace("+00:00", "Z"), d1.isoformat().replace("+00:00", "Z")

# ---------- URLè§£æ ----------
def extract_from_url(url):
    if url.startswith("@"):
        return ("handle", url)
    u = urlparse(url)
    path = (u.path or "").strip("/")

    m = re.match(r"channel/(UC[\w-]{20,})", path)
    if m:
        return ("channel_id", m.group(1))
    if path.startswith("@"):
        return ("handle", path.split("/")[0])
    m = re.match(r"user/([^/]+)", path)
    if m:
        return ("username", m.group(1))
    m = re.match(r"c/([^/]+)", path)
    if m:
        return ("custom", m.group(1))
    return ("custom", path.split("/")[0] if path else url)

def resolve_channel_id(kind, value):
    if kind == "channel_id":
        return value
    if kind == "handle":
        handle = value if value.startswith("@") else f"@{value}"
        res = YOUTUBE.channels().list(part="id", forHandle=handle).execute()
        items = res.get("items", [])
        return items[0]["id"] if items else None
    if kind == "username":
        res = YOUTUBE.channels().list(part="id", forUsername=value).execute()
        items = res.get("items", [])
        return items[0]["id"] if items else None
    # custom ã‚„æ›–æ˜§ãªæ–‡å­—åˆ—ã¯æ¤œç´¢ã§è§£æ±º
    res = YOUTUBE.search().list(part="snippet", q=value, type="channel", maxResults=1).execute()
    items = res.get("items", [])
    return items[0]["snippet"]["channelId"] if items else None

# ---------- ãƒãƒ£ãƒ³ãƒãƒ«åŸºæœ¬æƒ…å ± ----------
def get_channel_info(channel_id):
    res = YOUTUBE.channels().list(part="snippet,statistics", id=channel_id, maxResults=1).execute()
    items = res.get("items", [])
    if not items:
        return None, None
    it = items[0]
    title = it["snippet"]["title"]
    stats = it["statistics"]
    subs = None if stats.get("hiddenSubscriberCount") else int(stats.get("subscriberCount", 0))
    return title, subs

# ---------- æœŸé–“å†…ã®å‹•ç”»IDã‚’åˆ—æŒ™ ----------
def list_video_ids_in_period(channel_id, published_after_iso, published_before_iso, max_pages=50):
    """
    æœŸé–“ã§çµã£ã¦å‹•ç”»IDã‚’åé›†ã€‚max_pagesã¯ä¿é™ºï¼ˆå¤šã™ãã‚‹éš›ã®ä¸Šé™ï¼‰ã€‚
    """
    ids = []
    page_token = None
    pages = 0
    while True:
        req = YOUTUBE.search().list(
            part="id",
            channelId=channel_id,
            type="video",
            order="date",
            publishedAfter=published_after_iso,
            publishedBefore=published_before_iso,
            maxResults=50,
            pageToken=page_token
        )
        res = req.execute()
        for it in res.get("items", []):
            vid = it["id"].get("videoId")
            if vid:
                ids.append(vid)
        page_token = res.get("nextPageToken")
        pages += 1
        if not page_token or pages >= max_pages:
            break
        time.sleep(0.15)  # å©ãã™ãé˜²æ­¢
    return ids

# ---------- è¦–è´å›æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’ã¾ã¨ã‚ã¦å–å¾— ----------
def fetch_views_comments(video_ids):
    """
    returns: (view_counts:list[int], comment_counts:list[int])
    ã‚³ãƒ¡ãƒ³ãƒˆç„¡åŠ¹ã¯é™¤å¤–ï¼ˆcommentCountæ¬ è½ï¼‰ã€‚
    """
    views, comments = [], []
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        res = YOUTUBE.videos().list(part="statistics", id=",".join(chunk), maxResults=50).execute()
        for v in res.get("items", []):
            st = v.get("statistics", {})
            if "viewCount" in st:
                try:
                    views.append(int(st["viewCount"]))
                except ValueError:
                    pass
            if "commentCount" in st:
                try:
                    comments.append(int(st["commentCount"]))
                except ValueError:
                    pass
        time.sleep(0.15)
    return views, comments

def mean_or_zero(nums):
    return (sum(nums) / len(nums)) if nums else 0.0

def main():
    # æœŸé–“ã‚’RFC3339 (UTC) ã«å¤‰æ›
    after_iso, before_iso = to_rfc3339_utc(DATE_FROM, DATE_TO)

    # å…¥åŠ›URLä¸€è¦§
    with open(INPUT_CSV, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        urls = [row["channel_url"].strip() for row in reader if row.get("channel_url")]

    results = []
    for url in urls:
        print(f"â–¶ {url}")
        kind, val = extract_from_url(url)
        channel_id = resolve_channel_id(kind, val)
        if not channel_id:
            print("  âŒ ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å®šå¤±æ•—")
            continue

        title, subs = get_channel_info(channel_id)
        if not title:
            print("  âŒ æƒ…å ±å–å¾—å¤±æ•—")
            continue

        # æœŸé–“å†…ã®å‹•ç”»ã‚’å–å¾—
        video_ids = list_video_ids_in_period(channel_id, after_iso, before_iso)
        uploads = len(video_ids)

        # è¦–è´å›æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’é›†è¨ˆ
        views, comments = fetch_views_comments(video_ids)
        avg_views = mean_or_zero(views)
        avg_comments = mean_or_zero(comments)

        subs_text = "(éå…¬é–‹)" if subs is None else f"{subs:,}"
        print(f"  âœ… {title}")
        print(f"     ç™»éŒ²è€…æ•°: {subs_text}")
        print(f"     ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æœ¬æ•°: {uploads}")
        print(f"     å¹³å‡è¦–è´å›æ•°: {avg_views:.2f}ï¼ˆå¯¾è±¡{len(views)}æœ¬ï¼‰")
        print(f"     å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {avg_comments:.2f}ï¼ˆå¯¾è±¡{len(comments)}æœ¬ï¼‰")

        # ==== å‡ºåŠ›CSVã¯æ¨™æº–å‡ºåŠ›ã¨åŒã˜æƒ…å ±ã ã‘ ====
        results.append({
            "channel_url": url,
            "channelTitle": title,
            "subscriberCount": subs if subs is not None else "",
            "uploads": uploads,
            "avgViewCount": f"{avg_views:.2f}",
            "avgCommentCount": f"{avg_comments:.2f}",
        })

    if results:
        with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=results[0].keys())
            w.writeheader()
            w.writerows(results)
        print(f"\nğŸ“„ {OUTPUT_CSV} ã« {len(results)}ä»¶ã‚’æ›¸ãå‡ºã—ã¾ã—ãŸ")
    else:
        print("âŒ å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ãªã—")

if __name__ == "__main__":
    main()

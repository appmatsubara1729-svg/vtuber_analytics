from __future__ import annotations
import csv, time, re, os, sys, json
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional, Tuple
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ====== è¨­å®šï¼ˆã“ã“ã ã‘ç·¨é›†ï¼‰ ======
DATE_FROM  = "2025-09-30"   # yyyy-mm-ddï¼ˆå†…éƒ¨ã§ publishedAfter ã«å¤‰æ›ï¼‰
DATE_TO    = "2025-10-06"   # yyyy-mm-ddï¼ˆå½“æ—¥å«ã‚€ï¼‰
INPUT_CSV  = "vtuber_list.csv"
OUTPUT_BASENAME = "vtuber_analytics_summary.csv"  # â†å¸¸ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ã§ä¿å­˜
# =================================

JST = timezone(timedelta(hours=9))

# ---------- APIã‚­ãƒ¼ãƒ»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚¿ ----------
class ApiKeyRotator:
    def __init__(self, env_path="./environment.json"):
        data = json.load(open(env_path, "r"))
        keys = []
        if isinstance(data.get("vtuber_analytics_api_keys"), list):
            keys = [k for k in data["vtuber_analytics_api_keys"] if k]
        elif data.get("vtuber_analytics_api_key"):
            keys = [data["vtuber_analytics_api_key"]]
        if not keys:
            raise RuntimeError("environment.json ã« API ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        self.keys = keys
        self.idx = 0
        self._client = build("youtube", "v3", developerKey=self.keys[self.idx])

    def client(self):
        return self._client

    def next(self) -> bool:
        """æ¬¡ã®ã‚­ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚ã‚‚ã†ç„¡ã‘ã‚Œã° Falseã€‚"""
        if self.idx + 1 >= len(self.keys):
            return False
        self.idx += 1
        self._client = build("youtube", "v3", developerKey=self.keys[self.idx])
        print(f"ğŸ” APIã‚­ãƒ¼ã‚’åˆ‡æ›¿ãˆã¾ã—ãŸï¼ˆ{self.idx+1}/{len(self.keys)}ï¼‰", file=sys.stderr)
        return True

rotator = ApiKeyRotator()  # â† ã“ã“ã§ environment.json ã‚’èª­ã¿è¾¼ã‚€

def is_quota_error(e: HttpError) -> bool:
    try:
        if hasattr(e, "resp") and getattr(e.resp, "status", None) == 403:
            msg = e.content.decode("utf-8", errors="ignore")
            return ("quotaExceeded" in msg) or ("dailyLimitExceeded" in msg)
    except Exception:
        pass
    return False

def with_quota_rotation(make_request):
    """
    make_request = lambda yt: yt.XXXXX().list(...).execute() ã‚’è¿”ã™â€œç›´å‰â€ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆãƒ©ãƒ ãƒ€ã€‚
    ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ãªã‚‰æ¬¡ã®APIã‚­ãƒ¼ã«åˆ‡æ›¿ãˆã¦è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã€‚
    """
    while True:
        try:
            req = make_request(rotator.client())
            return req.execute()
        except HttpError as e:
            if is_quota_error(e):
                if not rotator.next():
                    raise  # ã‚­ãƒ¼ãŒå°½ããŸ
                # åˆ‡æ›¿ãˆã¦å†è©¦è¡Œ
                continue
            raise

# ---------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def to_rfc3339_utc(date_from: str, date_to: str) -> Tuple[str, str]:
    d0 = datetime.strptime(date_from, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    d1 = datetime.strptime(date_to, "%Y-%m-%d").replace(tzinfo=timezone.utc) + timedelta(days=1)
    return d0.isoformat().replace("+00:00", "Z"), d1.isoformat().replace("+00:00", "Z")

def output_path_with_timestamp(base_name: str) -> str:
    ts = datetime.now().strftime("%Y-%m-%d %H-%M-%S")  # Windowsäº’æ›
    root, ext = os.path.splitext(base_name)
    return f"{root}_{ts}{ext or '.csv'}"

def write_results(rows: List[Dict[str, Any]], path: str, columns: List[str]):
    if not rows:
        return
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=columns)
        w.writeheader()
        for r in rows:
            w.writerow({k: r.get(k, "") for k in columns})

def chunked(seq: List[str], n: int):
    for i in range(0, len(seq), n):
        yield seq[i:i+n]

# ---------- URL â†’ channelIdï¼ˆãƒ«ãƒ¼ãƒ«å„ªå…ˆ & æ¤œç´¢ã¯æœ€å¾Œï¼‰ ----------
_chid_cache: Dict[str, Optional[str]] = {}

def extract_from_url(url: str) -> Tuple[str, str]:
    if url.startswith("@"):
        return ("handle", url)

    u = urlparse(url)
    path = (u.path or "").strip("/")

    m = re.match(r"^channel/(UC[\w-]{20,})$", path)
    if m: return ("channel_id", m.group(1))
    if path.startswith("@"): return ("handle", path.split("/")[0])
    m = re.match(r"^user/([^/]+)$", path)
    if m: return ("username", m.group(1))
    m = re.match(r"^c/([^/]+)$", path)
    if m: return ("custom", m.group(1))

    if path:
        seg = path.split("/")[0]
        if seg.startswith("@"):
            return ("handle", seg)
        return ("custom", seg)

    q = parse_qs(u.query or "")
    if "channel" in q and q["channel"]:
        val = q["channel"][0]
        if val.startswith("UC"):
            return ("channel_id", val)

    return ("guess", url)

def resolve_channel_id(url: str) -> Optional[str]:
    if url in _chid_cache:
        return _chid_cache[url]
    kind, value = extract_from_url(url)
    chid: Optional[str] = None
    try:
        if kind == "channel_id":
            chid = value
        elif kind == "handle":
            handle = value if value.startswith("@") else f"@{value}"
            res = with_quota_rotation(lambda yt: yt.channels().list(part="id", forHandle=handle, maxResults=1))
            items = res.get("items", [])
            if items: chid = items[0]["id"]
        elif kind == "username":
            res = with_quota_rotation(lambda yt: yt.channels().list(part="id", forUsername=value, maxResults=1))
            items = res.get("items", [])
            if items: chid = items[0]["id"]
        else:
            # custom/guess â†’ æœ€å¾Œã®æ‰‹æ®µã§ search.list
            res = with_quota_rotation(lambda yt: yt.search().list(part="snippet", q=value, type="channel", maxResults=1))
            items = res.get("items", [])
            if items: chid = items[0]["snippet"]["channelId"]
    except HttpError as e:
        print(f"[WARN] resolve_channel_id HttpError: {e}", file=sys.stderr)

    _chid_cache[url] = chid
    return chid

# ---------- ãƒãƒ£ãƒ³ãƒãƒ«åŸºæœ¬ ----------
def get_channel_info(channel_id: str) -> Tuple[Optional[str], Optional[int]]:
    res = with_quota_rotation(lambda yt: yt.channels().list(part="snippet,statistics", id=channel_id, maxResults=1))
    items = res.get("items", [])
    if not items: return None, None
    it = items[0]
    title = it["snippet"]["title"]
    stats = it["statistics"]
    subs = None if stats.get("hiddenSubscriberCount") else int(stats.get("subscriberCount", 0))
    return title, subs

# ---------- æœŸé–“å†…å‹•ç”» ----------
def list_video_ids_in_period(channel_id: str, after_iso: str, before_iso: str, max_pages=50) -> List[str]:
    ids: List[str] = []
    token, pages = None, 0
    while True:
        res = with_quota_rotation(lambda yt: yt.search().list(
            part="id",
            channelId=channel_id,
            type="video",
            order="date",
            publishedAfter=after_iso,
            publishedBefore=before_iso,
            maxResults=50,
            pageToken=token
        ))
        for it in res.get("items", []):
            vid = it["id"].get("videoId")
            if vid: ids.append(vid)
        token = res.get("nextPageToken")
        pages += 1
        if not token or pages >= max_pages:
            break
        time.sleep(0.12)
    return ids

def fetch_video_stats(video_ids: List[str]) -> Tuple[List[int], List[int]]:
    views, comments = [], []
    for batch in chunked(video_ids, 50):
        res = with_quota_rotation(lambda yt: yt.videos().list(part="statistics", id=",".join(batch), maxResults=50))
        for v in res.get("items", []):
            st = v.get("statistics", {})
            if "viewCount" in st:
                try: views.append(int(st["viewCount"]))
                except: pass
            if "commentCount" in st:
                try: comments.append(int(st["commentCount"]))
                except: pass
        time.sleep(0.12)
    return views, comments

def mean_or_zero(nums: List[int]) -> float:
    return (sum(nums)/len(nums)) if nums else 0.0

# ---------- ãƒ¡ãƒ³é™ï¼ˆUUMOï¼‰ ----------
def members_only_playlist_id(channel_id: str, category: str = "all") -> str:
    core = channel_id[2:]
    prefix = {"all":"UUMO","videos":"UUMF","shorts":"UUMS","live":"UUMV"}.get(category, "UUMO")
    return prefix + core

def fetch_playlist_items(playlist_id: str) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    token = None
    while True:
        res = with_quota_rotation(lambda yt: yt.playlistItems().list(
            part="snippet,contentDetails,status",
            playlistId=playlist_id,
            maxResults=50,
            pageToken=token
        ))
        items.extend(res.get("items", []))
        token = res.get("nextPageToken")
        if not token: break
        time.sleep(0.12)
    return items

def fetch_videos_details(video_ids: List[str]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for batch in chunked(video_ids, 50):
        # ãƒªãƒˆãƒ©ã‚¤ã¯ with_quota_rotation ãŒã‚„ã‚‹ã®ã§ã“ã“ã§ã¯ä¸è¦
        res = with_quota_rotation(lambda yt: yt.videos().list(
            part="snippet,statistics,status,contentDetails",
            id=",".join(batch),
            maxResults=50
        ))
        out.extend(res.get("items", []))
        time.sleep(0.12)
    return out

def now_jst() -> datetime:
    return datetime.now(JST)

def within_months(pub_jst: datetime, now: datetime, months: int) -> bool:
    start = now - timedelta(days=30*months)
    return (start <= pub_jst <= now)

def parse_published_at_utc(s: str) -> datetime:
    return datetime.strptime(s, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)

def safe_int(d: Dict[str, Any], key: str) -> Optional[int]:
    if key in d:
        try: return int(d[key])
        except: return None
    return None

def members_averages(channel_id: str) -> Tuple[int, float, float, int, float, float]:
    """
    (count_1m, avg_like_1m, avg_comment_1m,
     count_2m, avg_like_2m, avg_comment_2m)
    """
    try:
        plid = members_only_playlist_id(channel_id, "all")
        pl_items = fetch_playlist_items(plid)
        if not pl_items:
            return 0, 0.0, 0.0, 0, 0.0, 0.0

        seen, video_ids = set(), []
        for it in pl_items:
            vid = (it.get("contentDetails") or {}).get("videoId")
            if vid and vid not in seen:
                seen.add(vid); video_ids.append(vid)

        videos = fetch_videos_details(video_ids)
        now = now_jst()

        count_1m = count_2m = 0
        likes_1m, comms_1m = [], []
        likes_2m, comms_2m = [], []

        for v in videos:
            sn = v.get("snippet") or {}
            st = v.get("statistics") or {}
            pub = sn.get("publishedAt")
            if not pub: continue
            pub_jst = parse_published_at_utc(pub).astimezone(JST)
            in_1m = within_months(pub_jst, now, 1)
            in_2m = within_months(pub_jst, now, 2)

            if in_1m: count_1m += 1
            if in_2m: count_2m += 1

            like = safe_int(st, "likeCount")
            com  = safe_int(st, "commentCount")
            if in_1m:
                if like is not None: likes_1m.append(like)
                if com  is not None: comms_1m.append(com)
            if in_2m:
                if like is not None: likes_2m.append(like)
                if com  is not None: comms_2m.append(com)

        return (
            count_1m, mean_or_zero(likes_1m),  mean_or_zero(comms_1m),
            count_2m, mean_or_zero(likes_2m),  mean_or_zero(comms_2m),
        )
    except HttpError as e:
        print(f"[WARN] members_averages HttpError: {e}", file=sys.stderr)
        return 0,0.0,0.0, 0,0.0,0.0
    except Exception as e:
        print(f"[WARN] members_averages error: {e}", file=sys.stderr)
        return 0,0.0,0.0, 0,0.0,0.0

# ---------- ãƒ¡ã‚¤ãƒ³ ----------
def main():
    after_iso, before_iso = to_rfc3339_utc(DATE_FROM, DATE_TO)
    output_path = output_path_with_timestamp(OUTPUT_BASENAME)

    with open(INPUT_CSV, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        urls = [row["channel_url"].strip() for row in reader if row.get("channel_url")]

    # CSV/æ¨™æº–å‡ºåŠ›ã«è¼‰ã›ã‚‹åˆ—ï¼ˆ= å®Œå…¨ä¸€è‡´ï¼‰
    COLUMNS = [
        "channel_url",
        "channelTitle",
        "subscriberCount",
        "uploads",              # æŒ‡å®šæœŸé–“ã®å…¬é–‹æœ¬æ•°
        "avgViewCount",         # æŒ‡å®šæœŸé–“ã®å¹³å‡è¦–è´å›æ•°
        "avgCommentCount",      # æŒ‡å®šæœŸé–“ã®å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆæ•°
        "membersCount_1m",      # ãƒ¡ãƒ³é™1ãƒ¶æœˆã®æœ¬æ•°
        "membersAvgLike_1m",
        "membersAvgComment_1m",
        "membersCount_2m",      # ãƒ¡ãƒ³é™2ãƒ¶æœˆã®æœ¬æ•°
        "membersAvgLike_2m",
        "membersAvgComment_2m",
    ]

    results: List[Dict[str, Any]] = []

    for url in urls:
        print(f"â–¶ ãƒãƒ£ãƒ³ãƒãƒ«URL: {url}")
        try:
            chid = resolve_channel_id(url)
            if not chid:
                print("  âŒ ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å®šå¤±æ•—")
                continue

            title, subs = get_channel_info(chid)
            if not title:
                print("  âŒ æƒ…å ±å–å¾—å¤±æ•—")
                continue

            # æŒ‡å®šæœŸé–“ã®å…¬é–‹å‹•ç”»ä¸€è¦§â†’è¦–è´/ã‚³ãƒ¡ãƒ³ãƒˆé›†è¨ˆ
            vids = list_video_ids_in_period(chid, after_iso, before_iso)
            uploads = len(vids)
            views, comments = fetch_video_stats(vids)
            avg_views = mean_or_zero(views)
            avg_comments = mean_or_zero(comments)

            # ãƒ¡ãƒ³é™ï¼š1/2ãƒ¶æœˆï¼ˆæœ¬æ•°â†’å¹³å‡ï¼‰
            (count_1m, avg_like_1m, avg_comm_1m,
             count_2m, avg_like_2m, avg_comm_2m) = members_averages(chid)

            row = {
                "channel_url": url,
                "channelTitle": title,
                "subscriberCount": subs if subs is not None else "",
                "uploads": uploads,
                "avgViewCount": f"{avg_views:.2f}",
                "avgCommentCount": f"{avg_comments:.2f}",
                "membersCount_1m": count_1m,
                "membersAvgLike_1m": f"{avg_like_1m:.2f}",
                "membersAvgComment_1m": f"{avg_comm_1m:.2f}",
                "membersCount_2m": count_2m,
                "membersAvgLike_2m": f"{avg_like_2m:.2f}",
                "membersAvgComment_2m": f"{avg_comm_2m:.2f}",
            }
            results.append(row)

            # â€”â€” æ¨™æº–å‡ºåŠ›ã¯ CSV ã¨åŒã˜æƒ…å ±ã‚’æ—¥æœ¬èªã§ï¼ˆé †åºã‚‚æƒãˆã‚‹ï¼‰â€”â€”
            print(f"  âœ… ãƒãƒ£ãƒ³ãƒãƒ«å: {row['channelTitle']}")
            print(f"     ç™»éŒ²è€…æ•°: {row['subscriberCount'] if row['subscriberCount'] != '' else '(éå…¬é–‹)'}")
            print(f"     æœŸé–“å†…ã‚¢ãƒƒãƒ—æœ¬æ•°: {row['uploads']}")
            print(f"     æœŸé–“å†…å¹³å‡è¦–è´å›æ•°: {row['avgViewCount']} / æœŸé–“å†…å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {row['avgCommentCount']}")
            print(f"     [ãƒ¡ãƒ³é™] 1ãƒ¶æœˆ: æœ¬æ•°{row['membersCount_1m']} / ã„ã„ã­{row['membersAvgLike_1m']} / ã‚³ãƒ¡ãƒ³ãƒˆ{row['membersAvgComment_1m']}")
            print(f"     [ãƒ¡ãƒ³é™] 2ãƒ¶æœˆ: æœ¬æ•°{row['membersCount_2m']} / ã„ã„ã­{row['membersAvgLike_2m']} / ã‚³ãƒ¡ãƒ³ãƒˆ{row['membersAvgComment_2m']}")
            print()

            # éšœå®³æ™‚ã«å‚™ãˆã¦é€æ¬¡ä¿å­˜ï¼ˆå†…å®¹ã¯ä¸Šæ›¸ãã€ãƒ•ã‚¡ã‚¤ãƒ«åã¯å›ºå®šã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
            write_results(results, output_path, COLUMNS)

        except HttpError as e:
            # with_quota_rotation å†…ã§ã‚­ãƒ¼åˆ‡æ›¿â†’å°½ããŸã‚‰ã“ã“ã«è½ã¡ã¦ãã‚‹
            if is_quota_error(e):
                print("â›½ å…¨APIã‚­ãƒ¼ã®ã‚¯ã‚©ãƒ¼ã‚¿ãŒå°½ãã¾ã—ãŸã€‚ã“ã“ã¾ã§ã®çµæœã‚’æ›¸ãå‡ºã—ã¦çµ‚äº†ã—ã¾ã™ã€‚")
                write_results(results, output_path, COLUMNS)
                return
            else:
                print(f"  âŒ HttpError: {e}")
                continue
        except Exception as e:
            print(f"  âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # æœ€çµ‚ä¿å­˜
    write_results(results, output_path, COLUMNS)
    print(f"ğŸ“„ å‡ºåŠ›: {output_path}ï¼ˆ{len(results)}ä»¶ï¼‰")

if __name__ == "__main__":
    main()
